import calendar
import glob
import time
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
import statsmodels.api as sm
import warnings
import gcsfs
import tempfile
import os
import pandas as pd

#%% Return filtered Dataframe for selected hours
def select_hours_df(df, measurements_hours):
    """
    This function returns filtered dataframe for selected hours
    :return:
    :param df: a dataframe for filtering data by hours
    :param measurements_hours: a list containing hours for which data needs to be retained
    :return: filtered dataframe retaining observations for hours defined in the argument measurement hours
    """
    new_df = df[df['hour'].isin(measurements_hours)]
    return new_df


# %% Function to Generate time at an interval
def generate_time(start_time, end_time, pandas_frequency):
    """
    This function generates series of time stamps with a specified temporal frequency
    :param start_time: the start time for generating the time stamps
    :param end_time: the end time for the time stamps
    :param pandas_frequency: this is temporal frequency at which to generate time stamps
    :return: a dataframe consisting of timestamps
    """
    time_begin = pd.to_datetime(start_time, format='%Y-%m-%d-%H-%M-%S')
    time_end = pd.to_datetime(end_time, format='%Y-%m-%d-%H-%M-%S')
    # pandas frequency or temporal frequency codes see:
    # https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases
    time_sequence = pd.date_range(start=time_begin, end=time_end, freq=pandas_frequency)
    return time_sequence


# %% Get filenames for measurement_sites (site by site)
def get_filenames(concentration_data_path, trace_gas_type,
                  site, inlet_height):
    """
    This function takes the directory where all the input files are stored and based on specified parameter
    returns list of filenames
    :param concentration_data_path: The path where concentration csv files are stored
    :param trace_gas_type: type of trace gas co2,ch4 or co
    :param site: the in-situ site for which data is required
    :param inlet_height: the inlet height at which data is being collected for a site
    :return: a list of filenames matching the wildcard generated by using trace gas type, site and inlet height
    """
    try:
        matches = []
        # Remove any trailing slashes and then add a single slash
        clean_path = concentration_data_path.rstrip('/') + '/'
        filename_string = clean_path + site + '?' + '*' + trace_gas_type + '-*'
        filenames = glob.glob(filename_string)
        
        for match in filenames:
            if inlet_height in match:
                matches.append(match)
        
        return matches
        
    except Exception as e:
        print(f"Error in get_filenames: {str(e)}")
        return []


# %% Return Empty Data Frame with all dates and create column names for storing data
def concentration_empty_df(trace_gas_type, trace_gas_meas_units,
                           year_begin, year_end, time_frequency, measurement_sites):
    """
    This function takes input beginning and end year and generates time at regular interval based
    on temporal frequency specified in variable time_frequency
    :param trace_gas_type: this is trace gas type and can be co2,ch4 or co. At present only
     co2 and ch4 are supported
    :param trace_gas_meas_units: the scientific units in which a trace gas is measured
    :param year_begin: beginning year from which
    :param year_end: end of the year until which time stamps would be generated
    :param time_frequency: temporal interval at which time stamps would be generated
    :param measurement_sites: a list of names of measurement sites
    :return: an empty dataframe consisting of timestamps and separate column consisting of year, month, day, hour,
    minute and seconds and names of sites for identifying  concentration and standard deviation of concentrations.
    Note the empty dataframe is filled with nan values except columns specifying time
    """
    # Generate time sequence
    time_sequence = pd.date_range(start=datetime(year_begin, 1, 1, 0),
                                  end=datetime(year_end, 12, 31, 23), freq=time_frequency)
    # convert numeric time for intersection
    numeric_time = pd.to_datetime(time_sequence).astype('int64') / 10 ** 9

    # Generate empty data frame
    empty_data_frame = pd.DataFrame()
    empty_data_frame['datetime_UTC'] = time_sequence
    empty_data_frame['year'] = time_sequence.year
    empty_data_frame['month'] = time_sequence.month
    empty_data_frame['day'] = time_sequence.day
    empty_data_frame['hour'] = time_sequence.hour
    empty_data_frame['minute'] = time_sequence.minute
    empty_data_frame['numeric_time'] = numeric_time
    # Note the site data column are filled with nan values
    for i in measurement_sites:
        col_name_new = trace_gas_type + '_' + trace_gas_meas_units + '_' + i
        empty_data_frame[col_name_new] = np.nan

        col_name_new = trace_gas_type + '_SD_minutes' + '_' + i
        empty_data_frame[col_name_new] = np.nan

    return empty_data_frame


# %% Return a filled dataframe for a site based on filenames
def return_dataframe_site(filenames, trace_gas_type, trace_gas_meas_units):
    """
    This function loops through all the filenames for a site and collect result in a dataframe
    Note: the concentration files are stored by years and site name is not required as that
    is already part of the filenames that are generated by using function get_filenames
    :param filenames: filenames are filenames for a site
    :param trace_gas_type: type of trace gas co2,ch4 or co
    :param trace_gas_meas_units: the scientific units in which a trace gas is measured
    :return: the function returns a dataframe consisting date and two columns consisting
    concentrations and SD of the concentrations for a site
    """
    site_dataframe = pd.DataFrame()
    col_name_1 = trace_gas_type + '_' + trace_gas_meas_units
    col_name_2 = trace_gas_type + '_SD_minutes'

    for i in filenames:
        if i == filenames[0]:
            site_dataframe = pd.read_csv(i)
            site_dataframe = site_dataframe[['datetime_UTC',
                                             col_name_1, col_name_2]]
        else:
            df2 = pd.read_csv(i)
            df2 = df2[['datetime_UTC', col_name_1, col_name_2]]
            site_dataframe = pd.concat([site_dataframe, df2],
                                       ignore_index=True)
    site_dataframe = site_dataframe.dropna()
    return site_dataframe


# %% Read Files and Create a Dataframe
# create an empty dataframe for storing data

def return_filled_dataframe(site_dataframe, empty_data_frame, trace_gas_type,
                            trace_gas_meas_units, site):
    """
    This function takes a dataframe for a site obtained from a site return_dataframe_site
    and put that in column specified for that site in empty data frame. Calling this function
    multiple times with different site_dataframe will completely fill the empty dataframe
    Note: the concentration files are stored by years and site name is not required as that
    is already part of the filenames that are generated by using function get_filenames
    :param site_dataframe: filled dataframe for a site obtained by using the function return
    dataframe site
    :param empty_data_frame: this is an empty dataframe which has columns consisting of
    concentrations and standard deviation for each site that would be filled
    :param trace_gas_type: type of trace gas co2,ch4 or co
    :param trace_gas_meas_units: the scientific units in which a trace gas is measured
    :param site: site name or code should match how site names are specified in filenames
    :return: a filled dataframe consisting data from a site
    """
    numeric_time_site = pd.DatetimeIndex(site_dataframe['datetime_UTC'])
    numeric_time_full = pd.DatetimeIndex(empty_data_frame['datetime_UTC'])

    # convert utc time to integer time and intersect them to get time periods for which data is available in
    # site_dataframe
    numeric_time_site = pd.to_datetime(numeric_time_site).astype('int64') / 10 ** 9
    numeric_time_full = pd.to_datetime(numeric_time_full).astype('int64') / 10 ** 9

    xy, x_ind, y_ind = np.intersect1d(numeric_time_full, numeric_time_site, return_indices=True)

    if len(x_ind) > 0 and len(y_ind) > 0:
        col_name_empty = trace_gas_type + '_' + trace_gas_meas_units + '_' + site
        col_name_site = trace_gas_type + '_' + trace_gas_meas_units
        index_col_x_ind = empty_data_frame.columns.get_loc(col_name_empty)
        index_col_y_ind = site_dataframe.columns.get_loc(col_name_site)
        empty_data_frame.iloc[x_ind, [index_col_x_ind]] = site_dataframe.iloc[y_ind, [index_col_y_ind]]

        col_name_empty = trace_gas_type + '_SD_minutes' + '_' + site
        col_name_site = trace_gas_type + '_SD_minutes'
        index_col = empty_data_frame.columns.get_loc(col_name_empty)
        index_col_y_ind = site_dataframe.columns.get_loc(col_name_site)
        empty_data_frame.iloc[x_ind, [index_col]] = site_dataframe.iloc[y_ind, [index_col_y_ind]]

    return empty_data_frame


# %% Collect Data for All Sites
def collect_allsite_data(concentration_data_path, trace_gas_type,
                         trace_gas_meas_units, year_begin, year_end,
                         time_frequency, measurement_sites, inlet_height):
    """
    This main goal of this function is to collect results (concentrations and standard deviation) for all
    sites and collect it in a single dataframe for plotting. It use all the previous functions to do this in
    order:

    concentration_empty_df (generates empty dataframe for storing data)
    get_filenames (get all filenames for a site)

    return_dataframe_site (get a dataframe filled with a data for a site. Note it is possible that data returned by this
    function has dates at unequal interval)
    return_filled_dataframe (this function takes dataframe returned by function return dataframe_site function
    and puts data in a dataframe that has time stamps at regular interval

    :param concentration_data_path: the path where the files are stored
    :param trace_gas_type: type of trace gas co2,ch4 or co
    :param trace_gas_meas_units: the scientific units in which a trace gas is measured ppb or ppm
    :param year_begin: beginning year for data for all sites
    :param year_end: end year for data for all sites
    :param time_frequency: temporal interval at which time stamps would be generated by using
    pandas temporal frequency codes see: https://pandas.pydata.org/docs/user_guide/timeseries.html
    timeseries-offset-aliases
    :param measurement_sites: a list of names of measurement sites
    :param inlet_height: a list of inlet height for which data is being collected. It is of same dimension as
    measurement sites
    :return:
    """
    counter = 0
    collect_data = None

    empty_data_frame = concentration_empty_df(trace_gas_type,
                                              trace_gas_meas_units,
                                              year_begin,
                                              year_end,
                                              time_frequency,
                                              measurement_sites)
    for i in measurement_sites:
        print(i)
        filenames = get_filenames(concentration_data_path,
                                  trace_gas_type,
                                  i, inlet_height[counter])
        site_dataframe = return_dataframe_site(filenames,
                                               trace_gas_type,
                                               trace_gas_meas_units)

        measurement_site = i
        collect_data = return_filled_dataframe(site_dataframe,
                                               empty_data_frame,
                                               trace_gas_type,
                                               trace_gas_meas_units,
                                               measurement_site)
        counter += 1
    return collect_data


# %% Aggregate Raw Data for All Sites
def aggregate_allsite_data(non_aggregated_data, trace_gas_type, time_frequency, stat_type='mean', units='ppm'):
    """
    Aggregates data from all sites based on the specified time frequency and statistical method.
    
    Parameters:
    -----------
    non_aggregated_data : pandas.DataFrame
        DataFrame with raw data at native resolution, must include 'datetime_UTC'
    trace_gas_type : str
        Type of trace gas (e.g., 'co2')
    time_frequency : str
        Desired temporal aggregation frequency (e.g., 'M' for monthly, 'D' for daily)
    stat_type : str, default='mean'
        Statistical method ('mean' or 'median')
    units : str, default='ppm'
        Units of the gas measurements ('ppm' or 'ppb')
    
    Returns:
    --------
    pandas.DataFrame
        Aggregated dataframe at the specified temporal resolution
    """
    # Input validation
    if stat_type not in ['mean', 'median']:
        raise ValueError("stat_type must be either 'mean' or 'median'")
    
    if units not in ['ppm', 'ppb']:
        raise ValueError("units must be either 'ppm' or 'ppb'")
    
    if 'datetime_UTC' not in non_aggregated_data.columns:
        raise ValueError("Input DataFrame must contain 'datetime_UTC' column")
    
    # Suppress empty slice warnings
    warnings.filterwarnings('ignore', category=RuntimeWarning, message='Mean of empty slice')
    
    # Set datetime as index
    df = non_aggregated_data.copy()
    df['datetime_UTC'] = pd.to_datetime(df['datetime_UTC'])
    df.set_index('datetime_UTC', inplace=True)
    
    # Extract relevant columns using the correct units
    raw_cols = list(df.filter(regex=f'{trace_gas_type}_{units}').columns)
    sd_cols = list(df.filter(regex=f'{trace_gas_type}_SD_minutes').columns)
    
    if not raw_cols:
        raise ValueError(f"No columns found matching pattern '{trace_gas_type}_{units}'")
    
    # Define aggregation functions
    def agg_raw(x):
        if stat_type == 'mean':
            return np.nanmean(x)
        return np.nanmedian(x)
    
    def agg_sd(x):
        return np.sqrt(np.nanmean(np.power(x, 2)))
    
    # Create aggregation dictionary
    agg_dict = {}
    agg_dict.update({col: agg_raw for col in raw_cols})
    agg_dict.update({col: agg_sd for col in sd_cols})
    
    # Perform resampling and aggregation
    resampled = df.resample(time_frequency).agg(agg_dict)
    
    resampled['year'] = resampled.index.year
    resampled['month'] = resampled.index.month
    resampled['day'] = resampled.index.day
    resampled['hour'] = resampled.index.hour
    resampled['minute'] = resampled.index.minute
    
    # Reset index to get datetime_UTC as a column
    resampled = resampled.reset_index()
    
    return resampled

#%% Aggregate Background Data

def aggregate_background_data(df, scale='MS', agg_method='mean'):
    """
    Aggregate time series data to different time scales with option for mean or median.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame with datetime_UTC as index and columns including CO2 measurements
    scale : str, default='MS'
        Time scale for aggregation. Options:
        - 'D': Daily
        - 'W': Weekly
        - 'MS': Month Start
    agg_method : str, default='mean'
        Method to aggregate CO2 measurements. Options:
        - 'mean': arithmetic mean
        - 'median': median value
        
    Returns:
    --------
    pandas.DataFrame
        Aggregated DataFrame with the specified time scale
    """
    # Input validation
    if agg_method not in ['mean', 'median']:
        raise ValueError("agg_method must be either 'mean' or 'median'")
    
    # Make sure datetime_UTC is datetime type
    if 'datetime_UTC' in df.columns:
        df = df.set_index('datetime_UTC')
    
    # Convert index to datetime if it's not already
    df.index = pd.to_datetime(df.index)
    
    # Identify CO2 measurement columns (those containing 'co2_ppm')
    co2_columns = [col for col in df.columns if 'co2_ppm' in col]
    
    # Separate numeric columns for aggregation
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # Create aggregation dictionary
    agg_dict = {}
    for col in df.columns:
        if col in numeric_cols:
            if col in co2_columns:
                agg_dict[col] = agg_method  # Use specified aggregation method for CO2
            elif col in ['year', 'month', 'day', 'hour', 'minute']:
                agg_dict[col] = 'first'  # Take first value for time components
            else:
                agg_dict[col] = 'mean'  # Default to mean for other numeric columns
    
    # Perform resampling and aggregation
    resampled = df.resample(scale).agg(agg_dict)
    
    # Update time components based on new index
    resampled['year'] = resampled.index.year
    resampled['month'] = resampled.index.month
    resampled['day'] = resampled.index.day
    resampled['hour'] = resampled.index.hour
    resampled['minute'] = resampled.index.minute
    
    # Reset index to get datetime_UTC as a column
    resampled = resampled.reset_index()
    
    return resampled
#%% Save and Read Data from HDF files. Note the Data should be stored at aggregated level in raw and background dictionary

def save_two_dicts_to_hdf(raw_dict, background_dict, hdf_filename):
    """
    Saves two nested dictionaries containing DataFrames into an HDF5 file.
    Supports both local paths and GCS paths.
    
    Parameters:
    -----------
    raw_dict : dict
        Dictionary containing nested DataFrames with structure {gas_type: {timeperiod: df}}
    background_dict : dict
        Dictionary containing nested DataFrames with structure {gas_type: {timeperiod: df}}
    hdf_filename : str
        Name of the HDF5 file to save the data (local path or GCS path starting with 'gs://')
    """
    # Initialize GCS filesystem if needed
    if hdf_filename.startswith('gs://'):
        fs = gcsfs.GCSFileSystem()
        # Create a temporary file for local operations
        with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:
            local_filename = temp_file.name
    else:
        local_filename = hdf_filename
    
    try:
        with pd.HDFStore(local_filename, mode='w', complevel=9, complib='zlib') as store:
            # Save raw dictionary
            for gas_type, time_dict in raw_dict.items():
                # Skip if time_dict is empty
                if not time_dict:
                    continue
                    
                for time_period, df in time_dict.items():
                    if isinstance(df, (pd.DataFrame, pd.Series)):
                        store.put(f"raw/{gas_type}/{time_period}", df)
            
            # Save background dictionary
            for gas_type, time_dict in background_dict.items():
                # Skip if time_dict is empty
                if not time_dict:
                    continue
                    
                for time_period, df in time_dict.items():
                    if isinstance(df, (pd.DataFrame, pd.Series)):
                        store.put(f"background/{gas_type}/{time_period}", df)
        
        # Upload to GCS if needed
        if hdf_filename.startswith('gs://'):
            fs.put(local_filename, hdf_filename)
            
    finally:
        # Clean up temporary file if we created one
        if hdf_filename.startswith('gs://') and 'local_filename' in locals():
            try:
                os.unlink(local_filename)
            except OSError:
                pass

def load_two_dicts_from_hdf(hdf_filename):
    """
    Loads two nested dictionaries containing DataFrames from an HDF5 file.
    Supports both local paths and GCS paths.
    
    Parameters:
    -----------
    hdf_filename : str
        Name of the HDF5 file to load the data from (local path or GCS path starting with 'gs://')
    
    Returns:
    --------
    tuple : (raw_dict, background_dict)
        Two nested dictionaries containing the loaded DataFrames
    """
    raw_dict = {'co2': {}, 'ch4': {}, 'co': {}}
    background_dict = {'co2': {}, 'ch4': {}, 'co': {}}
    
    # Check if filename is a GCS path
    if hdf_filename.startswith('gs://'):
        # Initialize GCS filesystem
        fs = gcsfs.GCSFileSystem()
        
        # Check if file exists on GCS
        if not fs.exists(hdf_filename):
            print(f"Error: HDF5 file does not exist on GCS: {hdf_filename}")
            return raw_dict, background_dict
        
        # Create a temporary file to download the HDF5 data
        with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:
            try:
                # Download the file from GCS to temporary local file
                fs.get(hdf_filename, temp_file.name)
                local_filename = temp_file.name
                
            except Exception as e:
                print(f"Error downloading HDF5 file from GCS: {str(e)}")
                return raw_dict, background_dict
    else:
        # Use local file path directly
        local_filename = hdf_filename
        if not os.path.exists(local_filename):
            print(f"Error: Local HDF5 file does not exist: {local_filename}")
            return raw_dict, background_dict
    
    try:
        with pd.HDFStore(local_filename, mode='r') as store:
            # Get all keys in the HDF5 file
            keys = [key[1:] for key in store.keys()]  # Remove leading '/'
            
            # Load raw dictionary data
            raw_keys = [key for key in keys if key.startswith("raw/")]
            for key in raw_keys:
                # Parse the path to get gas type and time period
                _, gas_type, time_period = key.split('/')
                if gas_type in raw_dict:
                    raw_dict[gas_type][time_period] = store[f"/{key}"]
            
            # Load background dictionary data
            bg_keys = [key for key in keys if key.startswith("background/")]
            for key in bg_keys:
                # Parse the path to get gas type and time period
                _, gas_type, time_period = key.split('/')
                if gas_type in background_dict:
                    background_dict[gas_type][time_period] = store[f"/{key}"]
                    
    finally:
        # Clean up temporary file if we created one
        if hdf_filename.startswith('gs://') and 'local_filename' in locals():
            try:
                os.unlink(local_filename)
            except OSError:
                pass
    
    return raw_dict, background_dict


def save_four_dicts_to_hdf(raw_dict, background_dict, raw_ratio_dict, background_ratio_dict, hdf_filename):
    """
    Saves four nested dictionaries containing DataFrames into an HDF5 file.
    Supports both local paths and GCS paths.
    
    Parameters:
    -----------
    raw_dict : dict
        Dictionary containing nested DataFrames with structure {gas_type: {timeperiod: df}}
    background_dict : dict
        Dictionary containing nested DataFrames with structure {gas_type: {timeperiod: df}}
    raw_ratio_dict : dict
        Dictionary containing nested DataFrames with structure {ratio_type: {timeperiod: df}}
    background_ratio_dict : dict
        Dictionary containing nested DataFrames with structure {ratio_type: {timeperiod: df}}
    hdf_filename : str
        Name of the HDF5 file to save the data (local path or GCS path starting with 'gs://')
    """
    # Initialize GCS filesystem if needed
    if hdf_filename.startswith('gs://'):
        fs = gcsfs.GCSFileSystem()
        # Create a temporary file for local operations
        with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:
            local_filename = temp_file.name
    else:
        local_filename = hdf_filename
    
    try:
        with pd.HDFStore(local_filename, mode='w', complevel=9, complib='zlib') as store:
            # Save raw gas dictionary
            for gas_type, time_dict in raw_dict.items():
                if not time_dict:
                    continue
                for time_period, df in time_dict.items():
                    if isinstance(df, (pd.DataFrame, pd.Series)):
                        store.put(f"raw/{gas_type}/{time_period}", df)
            
            # Save background gas dictionary
            for gas_type, time_dict in background_dict.items():
                if not time_dict:
                    continue
                for time_period, df in time_dict.items():
                    if isinstance(df, (pd.DataFrame, pd.Series)):
                        store.put(f"background/{gas_type}/{time_period}", df)
            
            # Save raw ratio dictionary
            for ratio_type, time_dict in raw_ratio_dict.items():
                if not time_dict:
                    continue
                for time_period, df in time_dict.items():
                    if isinstance(df, (pd.DataFrame, pd.Series)):
                        store.put(f"raw_ratio/{ratio_type}/{time_period}", df)
            
            # Save background ratio dictionary
            for ratio_type, time_dict in background_ratio_dict.items():
                if not time_dict:
                    continue
                for time_period, df in time_dict.items():
                    if isinstance(df, (pd.DataFrame, pd.Series)):
                        store.put(f"background_ratio/{ratio_type}/{time_period}", df)
        
        # Upload to GCS if needed
        if hdf_filename.startswith('gs://'):
            fs.put(local_filename, hdf_filename)
            
    finally:
        # Clean up temporary file if we created one
        if hdf_filename.startswith('gs://') and 'local_filename' in locals():
            try:
                os.unlink(local_filename)
            except OSError:
                pass


def load_four_dicts_from_hdf(hdf_filename):
    """
    Loads four nested dictionaries containing DataFrames from an HDF5 file.
    Supports both local paths and GCS paths.
    
    Parameters:
    -----------
    hdf_filename : str
        Name of the HDF5 file to load the data from (local path or GCS path starting with 'gs://')
    
    Returns:
    --------
    tuple : (raw_dict, background_dict, raw_ratio_dict, background_ratio_dict)
        Four nested dictionaries containing the loaded DataFrames
    """
    # Initialize dictionaries with proper structure
    raw_dict = {'co2': {}, 'ch4': {}, 'co': {}}
    background_dict = {'co2': {}, 'ch4': {}, 'co': {}}
    raw_ratio_dict = {'co2_ch4': {}, 'co_co2': {}, 'co_ch4': {}}
    background_ratio_dict = {'co2_ch4': {}}
    
    # Check if filename is a GCS path
    if hdf_filename.startswith('gs://'):
        # Initialize GCS filesystem
        fs = gcsfs.GCSFileSystem()
        
        # Check if file exists on GCS
        if not fs.exists(hdf_filename):
            print(f"Error: HDF5 file does not exist on GCS: {hdf_filename}")
            return raw_dict, background_dict, raw_ratio_dict, background_ratio_dict
        
        # Create a temporary file to download the HDF5 data
        with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:
            try:
                # Download the file from GCS to temporary local file
                fs.get(hdf_filename, temp_file.name)
                local_filename = temp_file.name
                
            except Exception as e:
                print(f"Error downloading HDF5 file from GCS: {str(e)}")
                return raw_dict, background_dict, raw_ratio_dict, background_ratio_dict
    else:
        # Use local file path directly
        local_filename = hdf_filename
        if not os.path.exists(local_filename):
            print(f"Error: Local HDF5 file does not exist: {local_filename}")
            return raw_dict, background_dict, raw_ratio_dict, background_ratio_dict
    
    try:
        with pd.HDFStore(local_filename, mode='r') as store:
            # Get all keys in the HDF5 file
            keys = [key[1:] for key in store.keys()]  # Remove leading '/'
            
            # Load raw gas dictionary data
            raw_keys = [key for key in keys if key.startswith("raw/")]
            for key in raw_keys:
                parts = key.split('/')
                gas_type = parts[1]
                time_period = parts[2]
                if gas_type in raw_dict:
                    raw_dict[gas_type][time_period] = store[f"/{key}"]
            
            # Load background gas dictionary data
            bg_keys = [key for key in keys if key.startswith("background/")]
            for key in bg_keys:
                parts = key.split('/')
                gas_type = parts[1]
                time_period = parts[2]
                if gas_type in background_dict:
                    background_dict[gas_type][time_period] = store[f"/{key}"]
            
            # Load raw ratio dictionary data
            raw_ratio_keys = [key for key in keys if key.startswith("raw_ratio/")]
            for key in raw_ratio_keys:
                parts = key.split('/')
                ratio_type = parts[1]
                time_period = parts[2]
                if ratio_type in raw_ratio_dict:
                    raw_ratio_dict[ratio_type][time_period] = store[f"/{key}"]
            
            # Load background ratio dictionary data
            bg_ratio_keys = [key for key in keys if key.startswith("background_ratio/")]
            for key in bg_ratio_keys:
                parts = key.split('/')
                ratio_type = parts[1]
                time_period = parts[2]
                if ratio_type in background_ratio_dict:
                    background_ratio_dict[ratio_type][time_period] = store[f"/{key}"]
                    
    finally:
        # Clean up temporary file if we created one
        if hdf_filename.startswith('gs://') and 'local_filename' in locals():
            try:
                os.unlink(local_filename)
            except OSError:
                pass
    
    return raw_dict, background_dict, raw_ratio_dict, background_ratio_dict

def append_dict_to_hdf(new_dict, dict_name, hdf_filename):
    """
    Appends a new dictionary to an existing HDF5 file.
    
    Args:
        new_dict: Dictionary containing DataFrames to append
        dict_name: Name for the new dictionary in the HDF store
        hdf_filename: Name of the HDF5 file
        
    Example:
        # Append census data to existing file
        append_dict_to_hdf(
            census_agg_data, 
            'census', 
            'aggregated_data.h5'
        )
    """
    with pd.HDFStore(hdf_filename, mode='a') as store:
        for key, df in new_dict.items():
            if isinstance(df, pd.DataFrame):
                store.put(f"{dict_name}/{key}", df)
                
def load_dicts_from_hdf(filename, categories):
    """
    Load data from HDF file for specified categories (zip, census, custom).
    Now supports both local files and GCS paths.
    
    Args:
        filename (str): Path to the HDF file (local or GCS path starting with 'gs://')
        categories (list): List of categories to load (e.g., ['zip', 'census', 'custom'])
    
    Returns:
        dict: Dictionary containing the loaded data with prefixed keys
    """
    import gcsfs
    import tempfile
    import os
    
    results = {}
    
    # Check if filename is a GCS path
    if filename.startswith('gs://'):
        # Initialize GCS filesystem
        fs = gcsfs.GCSFileSystem()
        
        # Check if file exists on GCS
        if not fs.exists(filename):
            print(f"Error: HDF5 file does not exist on GCS: {filename}")
            return results
        
        # Create a temporary file to download the HDF5 data
        with tempfile.NamedTemporaryFile(suffix='.h5', delete=False) as temp_file:
            try:
                # Download the file from GCS to temporary local file
                fs.get(filename, temp_file.name)
                local_filename = temp_file.name
                
            except Exception as e:
                print(f"Error downloading HDF5 file from GCS: {str(e)}")
                return results
    else:
        # Use local file path directly
        local_filename = filename
        if not os.path.exists(local_filename):
            print(f"Error: Local HDF5 file does not exist: {local_filename}")
            return results
    
    try:
        for category in categories:
            try:
                # Define the paths for this category
                centroids_path = f'/{category}/{category}_centroids'
                est_flux_path = f'/{category}/{category}_est_flux'
                unc_flux_path = f'/{category}/{category}_unc_flux'
                
                # Read each component with prefixed keys
                with pd.HDFStore(local_filename, mode='r') as store:
                    if centroids_path in store:
                        results[f'{category}_centroids'] = store[centroids_path]
                    if est_flux_path in store:
                        results[f'{category}_est_flux'] = store[est_flux_path]
                    if unc_flux_path in store:
                        results[f'{category}_unc_flux'] = store[unc_flux_path]
                    
                if not any(k.startswith(category) for k in results):
                    print(f"Warning: No data found for category '{category}'")
                    
            except Exception as e:
                print(f"Error loading data for category '{category}': {str(e)}")
                continue
                
    finally:
        # Clean up temporary file if we created one
        if filename.startswith('gs://') and 'local_filename' in locals():
            try:
                os.unlink(local_filename)
            except OSError:
                pass  # File might already be deleted
    
    return results
# Example usage:
# spatial_data = load_dicts_from_hdf(spatial_hdf_filename, ['zip', 'census', 'custom'])

# %%
def rearrange_by_year(aggregated_data, filter_suffix, site_name):
    unique_values = aggregated_data["year"].unique()
    storage = np.zeros(len(unique_values))
    counter = 0
    for i in unique_values:
        extracted_dataframe = aggregated_data[aggregated_data['year'] == i]
        storage[counter] = len(extracted_dataframe.index)
        counter += 1

    column_names = unique_values.tolist()
    for i in range(0, len(unique_values)):
        column_names[i] = str(unique_values[i])

    rearranged_data = pd.DataFrame(columns=column_names, index=range(int(max(storage))))
    variable_name = filter_suffix + '_' + site_name
    for i in range(0, len(unique_values)):
        yearly_extract = aggregated_data[aggregated_data['year'] == unique_values[i]]
        convert_array = yearly_extract[variable_name]
        rearranged_data.loc[0:storage[i] - 1, column_names[i]] = convert_array.to_numpy()

    return rearranged_data


# %% Return dataframe with correct names for auto correlation plot
def acf_return(dataframe_acf_compute, filter_suffix, measurement_sites):
    """
    This function uses data stored in dataframe dataframe_acf_compute for computing auto correlation function
    for each site. This function mostly uses data from aggregate_allsite_data to compute acf though it can also do the
    same from the dataframe obtained from function collect_allsite_data
    :param dataframe_acf_compute: This is the dataframe from which data would be extracted to compute acf
    :param filter_suffix: string to extract data for each site like concentrations or standard deviations
    :param measurement_sites: a list of names of measurement sites
    :return: a dataframe consisting acf values by measurement_sites
    """
    # dataframe_acf_compute=agg_results
    column_name_keys = list(dataframe_acf_compute.filter(regex=filter_suffix).head())
    process_names = column_name_keys[:]

    column_name_keys.append('datetime_UTC')
    dataframe_acf_compute = dataframe_acf_compute.loc[:, column_name_keys]

    dataframe_acf_compute.set_index(['datetime_UTC'])

    acf_compute = pd.DataFrame()

    for i in process_names:
        auto_correlation = sm.tsa.acf(dataframe_acf_compute[i], missing='drop')
        acf_compute[i] = auto_correlation

        acf_compute['lags'] = np.arange(0, len(acf_compute.axes[0]), dtype=int)

        replacement_keys = dict(zip(process_names, measurement_sites))
        acf_compute.rename(columns=replacement_keys, inplace=True)

    return acf_compute


# %% Helper function that collects all data , filter it by hours and aggregate it by time frequency
def data_return_gas(concentration_data_path, trace_gas_type, trace_gas_meas_units, year_begin, year_end,
                    measurement_sites: list, inlet_height: list, time_frequency, measurement_hours: list):
    """
    This is a helper function that collects all data , filter it by hours and aggregate it by time frequency
    :param concentration_data_path: The path where concentration csv files are stored
    :param trace_gas_type: this is trace gas type and can be co2,ch4 or co. At present only
     co2 and ch4 are supported
    :param trace_gas_meas_units: the scientific units in which a trace gas is measured
    :param year_begin: beginning year from which data is collected
    :param year_end:  end year from which data is collected
    :param measurement_sites: a list of names of measurement sites
    :param inlet_height: the inlet height at which data is being collected for a site
    :param time_frequency: temporal interval at which time stamps would be generated by using
    pandas temporal frequency codes see: https://pandas.pydata.org/docs/user_guide/timeseries.html
    timeseries-offset-aliases
    :param measurement_hours: a list containing hours for which data needs to be retained
    :return: on return three pandas dataframes unfiltered data for all hours, filtered data by hours for all sites
    and aggregated data by time_frequency for all hours
    """
    # get data at hourly interval
    data_all_sites_gas = collect_allsite_data(concentration_data_path, trace_gas_type,
                                              trace_gas_meas_units, year_begin, year_end,
                                              'H', measurement_sites, inlet_height)

    # filter hourly data for chosen measurement hours
    data_all_sites_hours = select_hours_df(data_all_sites_gas, measurement_hours)

    # aggregate measurement hours output at particular aggregation scale
    aggregated_results_gas = aggregate_allsite_data(data_all_sites_hours, trace_gas_type,
                                                    trace_gas_meas_units, year_begin, year_end,
                                                    time_frequency, measurement_sites, 'mean')

    return data_all_sites_gas, data_all_sites_hours, aggregated_results_gas


# %% Compute Gas Ratios
def compute_ratios(data_frame_gas_a, data_frame_gas_b, filter_suffix_a, filter_suffix_b):
    """
    The main task of this function is to compute ratios of two gases
    :param data_frame_gas_a: data frame for gas a from which ratios would be computed
    :param data_frame_gas_b: data frame for gas b for which ratios would be be computed
    :param filter_suffix_a: filter string for gas a
    :param filter_suffix_b: filter string for gas b
    :return: a dataframe which has rations of gasb/gasa. If you want to do ratio gasa/gas b then swich the order
    """
    # Gas A
    column_name_keys = list(data_frame_gas_a.filter(regex=filter_suffix_a).head())
    process_names = column_name_keys[:]

    site_keys = {}
    for i in process_names:
        split_list = i.split('_')
        site_keys[i] = split_list[-1]

    column_name_keys.append('datetime_UTC')
    dataframe_gas_a = data_frame_gas_a.loc[:, column_name_keys]
    dataframe_gas_a = dataframe_gas_a.set_index(['datetime_UTC'])
    dataframe_gas_a = dataframe_gas_a.rename(columns=site_keys)

    # Gas B
    column_name_keys = list(data_frame_gas_b.filter(regex=filter_suffix_b).head())
    process_names = column_name_keys[:]

    # reset site keys for second gas
    site_keys = {}
    for i in process_names:
        split_list = i.split('_')
        site_keys[i] = split_list[-1]

    column_name_keys.append('datetime_UTC')
    data_frame_gas_b = data_frame_gas_b.loc[:, column_name_keys]
    data_frame_gas_b = data_frame_gas_b.set_index(['datetime_UTC'])
    data_frame_gas_b = data_frame_gas_b.rename(columns=site_keys)

    data_frame_ratio = data_frame_gas_b.div(dataframe_gas_a)
    data_frame_ratio['all_towers'] = data_frame_ratio.mean(axis=1, numeric_only=True)

    data_frame_ratio = data_frame_ratio.reset_index()
    dataframe_gas_a = dataframe_gas_a.reset_index()
    data_frame_gas_b = data_frame_gas_b.reset_index()

    return data_frame_ratio, dataframe_gas_a, data_frame_gas_b


# %% Convert to decimal date

def toYearFraction(date):
    def sinceEpoch(date):  # returns seconds since epoch
        return time.mktime(date.timetuple())

    s = sinceEpoch
    date_vector = np.nan * np.ones((len(date), 1))
    for i in range(len(date)):
        year = date[i].year
        startOfThisYear = datetime(year=year, month=1, day=1)
        startOfNextYear = datetime(year=year + 1, month=1, day=1)
        yearElapsed = s(date[i]) - s(startOfThisYear)
        yearDuration = s(startOfNextYear) - s(startOfThisYear)
        fraction = yearElapsed / yearDuration
        date_vector[i] = date[i].year + fraction

    return date_vector


# %% Convert from decimal date to datetime and numeric time

def fraction_date_todatetime(date):
    date_vector = pd.DataFrame(index=range(len(date)),
                               columns=['datetime_UTC', 'year', 'month', 'day', 'hour', 'minute', 'numeric_time'])
    for i in range(len(date)):
        year = int(date[i])
        d = timedelta((float(date[i]) - year) * (365 + int(calendar.isleap(year))))
        day_one = datetime(year, 1, 1)
        dtime = d + day_one
        date_vector.at[i, 'datetime_UTC'] = dtime.strftime("%Y-%m-%d %H:%M:%S")
        date_vector.at[i, 'year'] = dtime.year
        date_vector.at[i, 'month'] = dtime.month
        date_vector.at[i, 'day'] = dtime.day
        date_vector.at[i, 'hour'] = dtime.hour
        date_vector.at[i, 'minute'] = dtime.minute
    date_vector['numeric_time'] = pd.to_datetime(date_vector['datetime_UTC']).values.astype('int64') / 10 ** 9
    return date_vector

